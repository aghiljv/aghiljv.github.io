__NUXT_JSONP__("/blog/Sensor%20Fusion", (function(a,b,c,d,e,f,g,h,i,j){return {data:[{blog:{slug:g,name:g,titleImage:"220321\u002Ftitle",briefdesc:h,postDate:"March 21, 2021",blogNumber:51,toc:[{depth:2,text:i}],body:{type:"root",children:[{type:b,tag:e,props:{},children:[{type:a,value:"\n  Sensor fusion is combining of sensory data or data derived from disparate\n  sources such that the resulting information has less uncertainty than would be\n  possible when these sources were used individually. The resulting model is\n  more accurate because it balances the strengths of the different sensors.\n  Systems can then use the information provided through sensor fusion to support\n  more-intelligent actions.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  The data sources for a fusion process are not specified to originate from\n  identical sensors. One can distinguish direct fusion, indirect fusion and\n  fusion of the outputs of the former two.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:b,tag:f,props:{},children:[{type:a,value:"Direct fusion"}]},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  Direct fusion is the fusion of sensor data from a set of heterogeneous or\n  homogeneous sensors, soft sensors and history values of sensor data.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:b,tag:f,props:{},children:[{type:a,value:"Indirect fusion"}]},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  Indirect fusion uses information sources like “a priori” knowledge about the\n  environment and human input.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  Each sensor type has inherent strengths and weaknesses. Radars are very strong\n  at accurately determining distance and speed - even in challenging weather\n  conditions - but cannot read street signs or distinguish the colour of a\n  stoplight. Cameras do very well reading signs or classifying objects, such as\n  pedestrians, bicyclists or other vehicles. However, they can easily be blinded\n  by dirt, sun, rain, snow or darkness. LiDARs can accurately detect objects,\n  but they do not have the range or affordability of cameras or radar.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  Sensor fusion brings the data from each of these sensor types together, using\n  software algorithms to provide the most comprehensive and therefore accurate,\n  environmental model possible. It can also correlate data pulled from inside\n  the cabin, through a process known as interior and exterior sensor fusion.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  A vehicle could use sensor fusion to fuse information from multiple sensors of\n  the same type as well - for instance, radar. This improves perception by\n  taking advantage of partially overlapping fields of view. As multiple radars\n  observe the environment around a vehicle, more than one sensor will detect\n  objects at the same time. Interpreted through perception software, detections\n  from those multiple sensors can be overlapped or fused, increasing the\n  detection probability and reliability of objects around the vehicle and\n  yielding a more accurate and reliable representation of the environment.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  It goes without saying, the more sensors on a vehicle, the more challenging\n  fusion becomes, but also the more opportunity exists to improve performance.\n  The process can be done more efficiently while making use of a centralised\n  domain controller. Some of the benefits include:\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:b,tag:f,props:{},children:[{type:a,value:"1. Reduced sensor sizes"}]},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  Not so long ago, the processing power to analyse sensor data to determine and\n  track objects has been packaged with the cameras or radars. With a centralised\n  domain controller present, sensor data can be collected from each sensor and\n  fused within, which will result in reduced sensor sizes as the sensors now\n  does not need to process the information that they collect.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:b,tag:f,props:{},children:[{type:a,value:"2. Increased data sharing"}]},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  With traditional systems, smart sensors process environmental inputs\n  independently, which means any decisions made when using the information are\n  only as good as what that individual sensor can see. Now, all the data coming\n  from the sensors is shared centrally, there is more opportunity for active\n  safety applications in the domain controller to make use of it. By applying\n  Artificial Intelligence (AI) tools, useful information could be extracted that\n  would otherwise be discarded.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:b,tag:f,props:{},children:[{type:a,value:"3. Reduced Latency"}]},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  The domain controller does not have to wait for the sensor to process data\n  before acting upon it. This could help in increased speed in response and thus\n  reduced latency. This will be a huge advantage in situations where even a\n  fraction of a second matters.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:c}]},{type:b,tag:"h2",props:{},children:[{type:a,value:i}]},{type:a,value:c},{type:b,tag:e,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  Disclaimer : The views and opinions expressed in the article belong solely to\n  the author, and not necessarily to the author's employer, organisation,\n  committee or other group or individual.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:b,tag:d,props:{},children:[]},{type:b,tag:d,props:{},children:[]}]},dir:"\u002Fblogs",path:"\u002Fblogs\u002FSensor Fusion",extension:".md",createdAt:j,updatedAt:j},title:g,description:h,ogImage:"\u002Fimg\u002Fblog\u002F220321\u002Ftitle.jpg",params:{slug:g}}],fetch:{},mutations:[["pageTitle\u002Fset","BLOG"]]}}("text","element","\n","br","p","b","Sensor Fusion","The method and advantages of using multiple sensors together.","By embracing a vehicle architecture that allows for a high number of sensors\n  and then synthesizes the data through sensor fusion, vehicles can become\n  smarter, faster.","2022-12-24T07:45:17.613Z")));