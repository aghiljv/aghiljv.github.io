__NUXT_JSONP__("/blog/Computer%20Vision", (function(a,b,c,d,e,f,g,h,i,j,k,l,m){return {data:[{blog:{name:g,titleImage:"301120\u002Ftitle",briefdesc:m,postDate:"November 29, 2020",toc:[],body:{type:"root",children:[{type:b,tag:e,props:{},children:[{type:a,value:"\n  Human vision is amazingly beautiful and complex. It all started billions of\n  years ago when small organisms developed a mutation that made them sensitive\n  to light.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  Fast forward to today, and there is an abundance of life on the planet which\n  all have very similar visual systems. They include eyes for capturing light,\n  receptors in the brain for accessing it, and a visual cortex for processing\n  it. Genetically engineered and balanced pieces of a system which help us do\n  things as simple as appreciating a sunrise.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  But this is really just the beginning. In the past 30 years, we've made even\n  more strides to extending this amazing visual ability, not just to ourselves,\n  but to machines as well. The first type of photographic camera was invented\n  around 1816 where a small box held a piece of paper coated with silver\n  chloride. When the shutter was open, the silver chloride would darken where it\n  was exposed to light. Now, 200 years later, we have much more advanced\n  versions of the system that can capture photos right into digital form. So,\n  we've been able to closely mimic how the human eye can capture light and\n  colour. But it's turning out that that was the easy part. Understanding what's\n  in the photo is much more difficult.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Consider this picture."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:f},{type:b,tag:i,props:{srcSet:["\u002Fimg\u002Fblog\u002F301120\u002Fflower.webp"],type:j},children:[]},{type:a,value:f},{type:b,tag:k,props:{style:l,src:"\u002Fimg\u002Fblog\u002F301120\u002Fflower.jpg",alt:"Flower"},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  A human brain can look at it and immediately know that it's a flower. Our\n  brains are cheating since we've got a couple million years worth of\n  evolutionary context to help immediately understand what this is.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  But a computer doesn't have that same advantage. To an algorithm, the image\n  looks like this.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:f},{type:b,tag:i,props:{srcSet:["\u002Fimg\u002Fblog\u002F301120\u002FflowerComputer.webp"],type:j},children:[]},{type:a,value:f},{type:b,tag:k,props:{style:l,src:"\u002Fimg\u002Fblog\u002F301120\u002FflowerComputer.jpg",alt:"Flower for Computer"},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  Just a massive array of integer values which represent intensities across the\n  colour spectrum. There's no context here, just a massive pile of data.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  It turns out that the context is the crux of getting algorithms to understand\n  image content in the same way that the human brain does. And to make this\n  work, we use an algorithm very similar to how the human brain operates using\n  machine learning.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  Machine learning allows us to effectively train the context for a data set so\n  that an algorithm can understand what all those numbers in a specific\n  organization actually represent. And what if we have images that are difficult\n  for a human to classify? Can machine learning achieve better accuracy?\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  Computer vision is taking on increasingly complex challenges and is seeing\n  accuracy that rivals humans performing the same image recognition tasks. But\n  like humans, these models aren't perfect. They do sometimes make mistakes.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  The specific type of neural network that accomplishes this is called a\n  convolutional neural network or CNN. CNNs work by breaking an image down into\n  smaller groups of pixels called a filter. Each filter is a matrix of pixels,\n  and the network does a series of calculations on these pixels comparing them\n  against pixels in a specific pattern the network is looking for.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  In the first layer of a CNN, it is able to detect high-level patterns like\n  rough edges and curves. As the network performs more convolutions, it can\n  begin to identify specific objects like faces and animals.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  How does a CNN know what to look for and if its prediction is accurate? This\n  is done through a large amount of labelled training data. When the CNN starts,\n  all of the filter values are randomized. As a result, its initial predictions\n  make little sense. Each time the CNN makes a prediction against labelled data,\n  it uses an error function to compare how close its prediction was to the\n  image's actual label. Based on this error or loss function, the CNN updates\n  its filter values and starts the process again. Ideally, each iteration\n  performs with slightly more accuracy.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  What if instead of analysing a single image, we want to analyse a video using\n  machine learning? At its core, a video is just a series of image frames. To\n  analyse a video, we can build on our CNN for image analysis. In still images,\n  we can use CNNs to identify features. But when we move to video, things get\n  more difficult since the items we're identifying might change over time. Or,\n  more likely, there's context between the video frames that's highly important\n  to labelling.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  For example, if there's a picture of a half full cardboard box, we might want\n  to label it packing a box or unpacking a box depending on the frames before\n  and after it. This is where CNNs come up lacking. They can only take into\n  account spatial features, the visual data in an image, but can't handle\n  temporal or time features - how a frame is related to the one before it.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  To address this issue, we have to take the output of our CNN and feed it into\n  another model which can handle the temporal nature of our videos. This type of\n  model is called a recurrent neural network or RNN. While a CNN treats groups\n  of pixels independently, an RNN can retain information about what it's already\n  processed and use that in its decision making. RNNs can handle many types of\n  input and output data.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  For example, we train the RNN by passing it a sequence of frame descriptions -\n  empty box, open box, closing box - and finally, a label - packing.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:f},{type:b,tag:i,props:{srcSet:["\u002Fimg\u002Fblog\u002F301120\u002FRNN.webp"],type:j},children:[]},{type:a,value:f},{type:b,tag:k,props:{style:l,src:"\u002Fimg\u002Fblog\u002F301120\u002FRNN.jpg",alt:"RNN"},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  As the RNN processes each sequence, it uses a loss or error function to\n  compare its predicted output with the correct label. Then it adjusts the\n  weights and processes the sequence again until it achieves a higher accuracy.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:f},{type:b,tag:i,props:{srcSet:["\u002Fimg\u002Fblog\u002F301120\u002Fwolf.webp"],type:j},children:[]},{type:a,value:f},{type:b,tag:k,props:{style:l,src:"\u002Fimg\u002Fblog\u002F301120\u002Fwolf.jpg",alt:"Wolf"},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  The challenge of these approaches to image and video models, however, is that\n  the amount of data we need to truly mimic human vision is incredibly large. If\n  we train our model to recognize a picture of a wolf, as long as we're given\n  this one picture with the same lighting, colour, angle, and shape, we can see\n  that it's a wolf. But if you change any of that or even just rotate the wolf,\n  the algorithm might not understand what it is anymore.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  Now, this is the big picture problem. To get an algorithm to truly understand\n  and recognize image content the way the human brain does, you need to feed it\n  incredibly large amounts of data of millions of objects across thousands of\n  angles all annotated and properly defined.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"\n  Disclaimer : The views and opinions expressed in the article belong solely to\n  the author, and not necessarily to the author's employer, organisation,\n  committee or other group or individual.\n"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[]},{type:b,tag:d,props:{},children:[]},{type:b,tag:d,props:{},children:[]}]},dir:"\u002Fblogs",path:"\u002Fblogs\u002FComputer Vision",extension:".md",slug:g,createdAt:"2020-11-29T03:34:03.865Z",updatedAt:"2020-11-29T03:55:28.984Z"},title:g,description:m,ogImage:"\u002Fimg\u002Fblog\u002F301120\u002Ftitle.jpg",params:{slug:g}}],fetch:{},mutations:[["pageTitle\u002Fset","BLOG"]]}}("text","element","\n","br","p","\n  ","Computer Vision","picture","source","image\u002Fwebp","img","height: 100%; width: 100%; object-fit: contain","How does a computer see the world")));